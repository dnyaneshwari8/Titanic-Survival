# Titanic Survival Prediction (Kaggle)

## ğŸ“Œ Project Overview
This project aims to predict whether a passenger survived the Titanic disaster using machine learning techniques.  
The dataset is taken from the famous **Kaggle Titanic: Machine Learning from Disaster** competition.

This project demonstrates a complete **end-to-end machine learning workflow**, including data cleaning, feature encoding, model training, evaluation, and Kaggle submission.

---

## ğŸ“Š Dataset
- Source: Kaggle Titanic Competition
- Files used:
  - `train.csv` â€“ contains features and target variable (`Survived`)
  - `test.csv` â€“ contains features only (no target)

âš ï¸ Note: Dataset files are **not included** in this repository due to Kaggleâ€™s data usage policy.

---

## ğŸ› ï¸ Tools & Technologies
- Python
- Pandas
- NumPy
- Scikit-learn
- Google Colab / Jupyter Notebook

---

## ğŸ” Features Used
- Pclass
- Sex
- Age
- SibSp
- Parch
- Fare
- Embarked (one-hot encoded)

---

## ğŸ§¹ Data Preprocessing
- Handled missing values:
  - Age â†’ filled with median
  - Embarked â†’ filled with most frequent value
  - Fare (test data) â†’ filled with median
- Encoded categorical variables:
  - Sex â†’ label encoding
  - Embarked â†’ one-hot encoding
- Ensured train and test datasets have aligned feature columns

---

## ğŸ¤– Model Used
- **Logistic Regression**
- Train-validation split used for local evaluation
- Final model trained on full training dataset

---

## ğŸ“ˆ Model Performance
- Validation Accuracy: ~81%
- Kaggle Public Score: **0.76**

---

## ğŸ Kaggle Submission
- Submission file contains only:
  - `PassengerId`
  - `Survived`

Kaggle Competition Link:  
https://www.kaggle.com/competitions/titanic

---

## ğŸ¯ Learning Outcomes
- Understanding of ML pipeline
- Handling missing data
- Feature encoding techniques
- Difference between validation data and Kaggle test data
- Real-world ML competition workflow

---

## ğŸ“Œ Future Improvements
- Feature engineering (FamilySize, IsAlone, Title extraction)
- Try advanced models like Random Forest or Gradient Boosting
- Hyperparameter tuning
- Cross-validation
